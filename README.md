Mini transformer-style language model built from scratch using Python and PyTorch.
It learns from a small text dataset to predict the next word and generate simple sentences.
This project demonstrates the core workflow of transformers: tokenization → embeddings → attention → prediction.
